{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the neighborhood and load its data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'Akademicheskij'\n",
    "district = gpd.read_file('../Data dive/dd2/{}/{}_district.geojson'.format(name,name))\n",
    "neigh_posts = pd.read_csv('social_media/{}/vk.csv'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group posts by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vk_users = pd.DataFrame({'post' : neigh_posts.groupby('userId').apply( lambda x: ' '.join(x['text']))}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many users are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extra_words = ['http','br','id','com','www', 'instagram', 'vsco', 'https', 'instasize','repost',\n",
    "              'whatsapp', 'вотсап', 'repostapp','маникюр', 'бровь', 'ресница', 'губа', 'instacollage', 'опубликовывать',\n",
    "                'фото', 'москва', 'moscow']\n",
    "def process_docs(docs):\n",
    "    \"\"\"\n",
    "    Function to process texts. Following are the steps we take:\n",
    "    \n",
    "    1. Text tokenization.\n",
    "    2. Removing numbers \n",
    "    3. Stopword and short words Removal.\n",
    "    4. Lemmatization and filter words by their length.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "    texts: Tokenized texts.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    texts: Pre-processed tokenized texts.\n",
    "    \"\"\"\n",
    "    m = Mystem()\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stops = stopwords.words('russian') + stopwords.words('english') + extra_words\n",
    "    \n",
    "    for idx in tqdm(range(len(docs))):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not any(c.isdigit() for c in token) and ('id' not in token or 'club' not in token or 'ru' not in token)] for doc in tqdm(docs)]\n",
    "    #Lemmatize words\n",
    "    docs = [[m.lemmatize(token)[0] for token in doc ] for doc in tqdm(docs)]\n",
    "    #Remove stopwords\n",
    "    docs = [[token for token in doc if token not in stops] for doc in tqdm(docs)]\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in tqdm(docs)]\n",
    "    return docs\n",
    "    \n",
    "def get_corpus(docs):\n",
    "    \n",
    "    \"\"\"Add bigrams to docs and create corpus and dictionary for training\n",
    "    \n",
    "    Args:\n",
    "        docs: list of tokenized and cleaned texts;\n",
    "    Returns:\n",
    "        corpus: list of lists of tuples, where first element of tuple is a word id\n",
    "        and the second is the count of that word in the whole corpus\n",
    "        dictionary: gensim.corpora.dictionary.Dictionary \n",
    "  \n",
    "    \"\"\"\n",
    "    \n",
    "    frequency = defaultdict(int)\n",
    "    for text in tqdm(docs):\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    texts = [[token for token in text if frequency[token] > 3] for text in tqdm(docs)]\n",
    "\n",
    "    #Take the bigram, if token is a bigram, add to document.\n",
    "    bigram = Phrases(texts, min_count = 20)\n",
    "    for idx in tqdm(range(len(texts))):\n",
    "        for token in bigram[texts[idx]]:\n",
    "            if '_' in token:\n",
    "                texts[idx].append(token)\n",
    "    \n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary(texts)\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    #dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in tqdm(texts)]\n",
    "    \n",
    "    print('Number of unique tokens: {}'.format(len(dictionary)))\n",
    "    print('Number of documents: {}'.format(len(corpus)))\n",
    "    \n",
    "    return corpus, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process texts to be ready for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = vk_users['post'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = process_docs(texts.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, dictionary = get_corpus(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "#The training model - we use online LDA model which allows to update the model \n",
    "#and the following parameters should be defined\n",
    "num_topics = 10 # number of topics\n",
    "chunksize = 1000 \n",
    "passes = 10\n",
    "iterations = 400\n",
    "eval_every = 10  #evaluate model perplexity.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha=0.001, update_every = 1, \\\n",
    "                       num_topics=num_topics,\\\n",
    "                       eval_every=eval_every, passes = passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pyLDAvis.gensim.prepare(model, corpus, dictionary); # visualize lda topics\n",
    "pyLDAvis.display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign topics to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docTopicProbMat = model[corpus]\n",
    "lda_users = vk_users.copy()\n",
    "lda_users['topics'] = docTopicProbMat\n",
    "vk_users['topic'] = lda_users['topics'].apply(lambda x :x[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign topics to their posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_posts['topic'] = neigh_posts['userId'].apply(lambda userId: vk_users.loc[vk_users['userId']==userId,'topic'].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the histogram of topic distribution per posts and per users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "center_lat = list(district.centroid[0].coords)[0][1]\n",
    "center_lon = list(district.centroid[0].coords)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_places = folium.Map([center_lat, center_lon], tiles='Stamen Toner', zoom_start=14,control_scale=True)\n",
    "\n",
    "#Define style for geojson objects\n",
    "style_function = lambda feature: dict(fillColor='#AECCAE',\n",
    "                                      color='#AECCAE',\n",
    "                                      weight=1,\n",
    "                                      opacity=0.3)\n",
    "\n",
    "# Adding Houses\n",
    "houses = gpd.read_file('../Data dive/dd2/{}/{}_chruchevki.geogson'.format(name, name))\n",
    "points = folium.features.GeoJson(houses,name='Khurshevki houses')\n",
    "map_places.add_child(points,name='Khurshevki houses')\n",
    "\n",
    "#Adding district\n",
    "polygon = folium.features.GeoJson(district, style_function=style_function,name='district boundary')\n",
    "map_places.add_child(polygon,name='district boundary')\n",
    "\n",
    "colormap_dict = {0.0: 'pink', 0.3: 'blue', 0.5: 'green',  1.0: 'red'}\n",
    "\n",
    "#Adding topics heatmaps\n",
    "for topic_id in [0, 1, 2]:\n",
    "    topic_coords = list(zip(neigh_posts[neigh_posts.topic == topic_id].lat, neigh_posts[neigh_posts.topic == topic_id].lon))\n",
    "\n",
    "    HeatMap(topic_coords,\n",
    "            name='Topic: {}'.format(topic_id),\n",
    "            radius=10, \n",
    "            min_opacity=0.8,\n",
    "            gradient={0.0: 'pink', 0.3: 'blue', 0.5: 'green',  1.0: 'red'}).add_to(map_places)\n",
    "\n",
    "\n",
    "    colormap = folium.LinearColormap(colors = colormap_dict.values())\n",
    "    colormap.caption = 'Topic: {}'.format(topic_id)\n",
    "\n",
    "#Switch between layers\n",
    "folium.LayerControl().add_to(map_places)\n",
    "map_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
